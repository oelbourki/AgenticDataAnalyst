# ü§ñ AgenticDataAnalyst

> An AI-powered data analysis platform that converts natural language queries into executable Python code and runs it securely.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-success.svg)](https://github.com)

**Fully functional and Streamlit Cloud compatible!** The system uses LangGraph for workflow orchestration, Google Gemini for code generation, and codibox for secure code execution with dual backend support (Host/Docker).

## ‚ú® Features

- üß† **Natural Language to Code**: Converts user queries into executable Python code using AI
- üîç **Intelligent File Selection**: Automatically selects relevant datasets using semantic search
- ‚ö° **Dual Backend Execution**: 
  - **Host backend** (default): Fast execution, works on Streamlit Cloud
  - **Docker backend** (optional): Secure, isolated execution for local development
- üìä **Visualization Generation**: Automatically creates charts and visualizations
- üîÑ **Error Recovery**: Automatic code refinement with up to 3 retry attempts
- ü§ñ **AI-Powered Metadata**: Generates dataset descriptions and summaries automatically
- üåê **Multiple Interfaces**: CLI, Streamlit web app, and programmatic API

## Project Structure

```
AgenticDataAnalyst/
‚îú‚îÄ‚îÄ agent_coder/              # AI workflow system
‚îÇ   ‚îú‚îÄ‚îÄ agents.py             # FileAccessAgent, CodeGenerationAgent, CodeExecutor
‚îÇ   ‚îú‚îÄ‚îÄ workflow.py           # LangGraph workflow definitions
‚îÇ   ‚îú‚îÄ‚îÄ simple_workflow.py    # Enhanced workflow with result processing
‚îÇ   ‚îú‚îÄ‚îÄ utils.py              # Utility functions for CSV analysis
‚îÇ   ‚îî‚îÄ‚îÄ main.py               # CLI interface
‚îú‚îÄ‚îÄ datasets/                 # Data files and metadata
‚îÇ   ‚îú‚îÄ‚îÄ *.csv                 # Sample datasets
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json         # Dataset descriptions
‚îú‚îÄ‚îÄ docker/                   # Docker configuration (optional)
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile            # Container definition
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt      # Python packages for container
‚îú‚îÄ‚îÄ scripts/                  # Setup and utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup_gemini.py       # Gemini API key setup
‚îÇ   ‚îú‚îÄ‚îÄ setup_docker.py       # Docker container setup
‚îÇ   ‚îú‚îÄ‚îÄ check_docker.py       # Docker status checker
‚îÇ   ‚îú‚îÄ‚îÄ download_datasets.py  # Download sample datasets
‚îÇ   ‚îî‚îÄ‚îÄ create_metadata.py    # Generate dataset metadata
‚îú‚îÄ‚îÄ streamlit_app.py          # Web dashboard
‚îú‚îÄ‚îÄ setup_project.py          # Main project setup script
‚îú‚îÄ‚îÄ requirements.txt          # All project dependencies
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üìã Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Usage](#-usage)
- [Configuration](#-configuration)
- [Streamlit Cloud Deployment](#-streamlit-cloud-deployment)
- [Troubleshooting](#-troubleshooting)
- [Development](#-development)
- [Contributing](#-contributing)

## üèóÔ∏è Architecture

### Core Components

1. **FileAccessAgent**: Selects appropriate data files based on user queries using semantic search
2. **CodeGenerationAgent**: Generates Python code using LLM (Gemini/OpenAI)
3. **CodeExecutor**: Executes code with dual backend support (Host/Docker) and error handling
4. **Workflow**: LangGraph-based orchestration of agents

### Execution Flow

```mermaid
graph TD
    A[User Query] --> B[FileAccessAgent]
    B --> C[CodeGenerationAgent]
    C --> D[CodeExecutor]
    D --> E{Success?}
    E -->|Yes| F[Result Processing]
    E -->|No| G[Error Refinement]
    G --> C
    F --> H[ResultAdapter]
    H --> I[Display Results]
```

**Text Flow:**
```
User Query
    ‚Üì
FileAccessAgent (selects dataset via semantic search)
    ‚Üì
CodeGenerationAgent (generates Python code using LLM)
    ‚Üì
CodeExecutor (executes code - Host or Docker backend)
    ‚Üì
Error Handling (automatic refinement if errors occur)
    ‚Üì
Result Processing (extracts images, CSV files, markdown)
    ‚Üì
ResultAdapter (formats results for display)
    ‚Üì
Display Results (Streamlit UI)
```

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- Google Gemini API key ([Get one here](https://aistudio.google.com/app/apikey))
- Docker (optional, only if using Docker backend)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/yourusername/AgenticDataAnalyst.git
   cd AgenticDataAnalyst
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
   This will install all dependencies including `codibox` from pip.

3. **Set up Gemini API key:**
   ```bash
   python scripts/setup_gemini.py
   ```
   Or create a `.env` file with:
   ```env
   GOOGLE_API_KEY=your_key_here
   ```
   For Streamlit Cloud, set the key in Streamlit secrets.

4. **Download sample datasets (optional):**
   ```bash
   python scripts/download_datasets.py
   ```

5. **Set up Docker container (optional, only if using Docker backend):**
   ```bash
   python scripts/setup_docker.py
   ```
   Or set `USE_DOCKER_BACKEND=true` environment variable.

### Running the Application

**Streamlit Web App (Recommended):**
```bash
streamlit run streamlit_app.py
```
Then open your browser to `http://localhost:8501`

**CLI Interface:**
```bash
python -m agent_coder.main
```

## üíª Usage

### Streamlit App

1. Start the app: `streamlit run streamlit_app.py`
2. Select a dataset from the sidebar (or upload your own CSV)
3. Enter a natural language query (e.g., "Show me sales trends by region")
4. View generated charts and results

### Programmatic Usage

```python
from agent_coder.simple_workflow import process_query, simple_coder

# Process a query
result = process_query(simple_coder, "Show me sales trends by region")

# Access results
print(result["messages"][-1].content)
```

### Example Queries

- "Create a scatter plot of sepal length vs sepal width colored by species"
- "Show me the survival rate by passenger class"
- "Visualize the age distribution of customers"
- "Compare average sales across different regions"

## ‚öôÔ∏è Configuration

### LLM Models

The system uses Google Gemini by default, with OpenAI as fallback:

| Model | Status | Use Case |
|-------|--------|----------|
| `gemini-2.5-flash-lite` | ‚úÖ Preferred | Default for all agents |
| `gpt-4o-mini` | üîÑ Fallback | Used if Gemini unavailable |

### Execution Backends

| Feature | Host Backend (Default) | Docker Backend (Optional) |
|---------|----------------------|-------------------------|
| **Speed** | ~0.5-2 seconds | ~2-5 seconds |
| **Streamlit Cloud** | ‚úÖ Works | ‚ùå Not supported |
| **Security** | ‚ö†Ô∏è User permissions | ‚úÖ Isolated container |
| **Setup** | ‚úÖ Auto-install deps | ‚ö†Ô∏è Requires Docker |
| **Network** | ‚úÖ Full access | ‚ùå No network access |

**To use Docker backend:**
```bash
export USE_DOCKER_BACKEND=true
```

## Dependencies

### Core Libraries
- `langchain` - LLM orchestration
- `langgraph` - Workflow management
- `langchain-google-genai` - Gemini integration
- `pandas`, `numpy` - Data manipulation
- `matplotlib`, `seaborn` - Visualization
- `streamlit` - Web interface

### Docker Container Packages
- Jupyter, nbconvert - Notebook execution
- Data science libraries (pandas, numpy, matplotlib, seaborn)
- Machine learning libraries (scikit-learn, statsmodels)

## Troubleshooting

### Import Errors

If you see `ImportError: No module named 'codibox'`:
- Install dependencies: `pip install -r requirements.txt`
- The codibox package should be installed automatically from pip

### Docker Container Issues (if using Docker backend)

Check container status:
```bash
python scripts/check_docker.py
```

Or manually:
```bash
docker ps -a | grep sandbox
docker start sandbox
```

### API Key Issues

Ensure your API key is set:
```bash
python scripts/setup_gemini.py
```

Or check `.env` file exists with `GOOGLE_API_KEY`.

For Streamlit Cloud:
- Set `GOOGLE_API_KEY` in Streamlit Cloud secrets dashboard

### Backend Selection

The app uses **Host backend by default** (works everywhere). To use Docker:
- Set environment variable: `export USE_DOCKER_BACKEND=true`
- Or in Streamlit Cloud secrets: `USE_DOCKER_BACKEND = "true"`

## ‚òÅÔ∏è Streamlit Cloud Deployment

The app is fully compatible with Streamlit Cloud! Follow these steps:

### Step 1: Push to GitHub
```bash
git add .
git commit -m "Initial commit"
git push origin main
```

### Step 2: Deploy on Streamlit Cloud
1. Go to [share.streamlit.io](https://share.streamlit.io)
2. Click "New app"
3. Connect your GitHub repository
4. Set main file path: `streamlit_app.py`

### Step 3: Configure Secrets
In Streamlit Cloud dashboard, go to Settings ‚Üí Secrets and add:
```toml
GOOGLE_API_KEY = "your-api-key-here"
```

### What Works Automatically
- ‚úÖ Works without Docker
- ‚úÖ Auto-installs dependencies
- ‚úÖ Uses Host backend by default
- ‚úÖ Handles secrets securely
- ‚úÖ No additional configuration needed

## üõ†Ô∏è Development

### Project Status

‚úÖ **Production Ready**: Fully functional and tested

### Current Features

- ‚úÖ Natural language to code generation
- ‚úÖ Dual backend support (Host/Docker)
- ‚úÖ Streamlit Cloud compatible
- ‚úÖ AI-powered metadata generation
- ‚úÖ Automatic error recovery
- ‚úÖ Image and CSV extraction

### Roadmap

- [ ] Add unit tests
- [ ] Improve error messages
- [ ] Add result caching
- [ ] Support for multiple file inputs
- [ ] Enhanced visualization options
- [ ] Performance optimizations

### Tech Stack

- **LLM**: Google Gemini 2.5 Flash Lite (OpenAI fallback)
- **Framework**: LangChain + LangGraph
- **Execution**: codibox (Host/Docker backends)
- **Web**: Streamlit
- **Data**: pandas, numpy, matplotlib, seaborn

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìû Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/AgenticDataAnalyst/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/AgenticDataAnalyst/discussions)

## ‚≠ê Star History

If you find this project useful, please consider giving it a star!

---

**Made by Otmane El Bourki**
